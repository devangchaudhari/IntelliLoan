# -*- coding: utf-8 -*-
"""LoanPred.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kXVKGo1-Ks9IHr45uJZmgbdjs1tGZVT0
"""

import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn.metrics import accuracy_score

df=pd.read_csv('/train_u6lujuX_CVtuZ9i.csv')

df.head()

df.shape

df.info()

df.describe()

df.isnull().sum()

df.interpolate(method='linear', inplace=True)

df.isnull().sum()

df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)

df['Married'].fillna(df['Married'].mode()[0], inplace=True)

df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)

df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)

df.isnull().sum()

df=df.dropna()

df.isnull().sum()

df

# label encoding
df.replace({"Loan_Status":{'N':0,'Y':1}},inplace=True)

df.head()

df['Dependents'].value_counts()

# Replcae 3+ with 4
df=df.replace(to_replace='3+',value=4)

df['Dependents'].value_counts()

df['Loan_Status'].value_counts().plot.bar()

"""## Data Visualization"""

#Education vs Loan Status
sns.countplot(x='Education',hue='Loan_Status',data=df)

# Maritial Status vs Loan-status

sns.countplot(x='Married',hue='Loan_Status',data=df)

# convert categorical columns to numerical values
df.replace({'Married':{'No':0,'Yes':1},'Gender':{'Male':1,'Female':0},'Self_Employed':{'No':0,'Yes':1},
                      'Property_Area':{'Rural':0,'Semiurban':1,'Urban':2},'Education':{'Graduate':1,'Not Graduate':0}},inplace=True)

df.head()

X=df.drop(columns=['Loan_ID','Loan_Status'],axis=1)

Y=df['Loan_Status']

X

Y

# Train Test Split

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1,stratify=Y,random_state=2)

print(X_train.shape,X_test.shape)

# Model - Support Vector Machaine Model

classifier=svm.SVC(kernel='linear')

#training

classifier.fit(X_train,Y_train)

X_train_pred=classifier.predict(X_train)
training_data_accuracy=accuracy_score(X_train_pred,Y_train)

print(training_data_accuracy)

X_test_pred=classifier.predict(X_test)
test_data_accuracy=accuracy_score(X_test_pred,Y_test)

print(test_data_accuracy)

import pickle

filename = 'loan_status_model.pkl'
pickle.dump(classifier, open(filename, 'wb'))

import matplotlib.pyplot as plt  # Importing matplotlib for plotting
import pandas as pd

# Assuming df is already defined and contains the data

# Crosstab for Gender and Loan_Status
Gender = pd.crosstab(df['Gender'], df['Loan_Status'])

# Normalize and plot
Gender.div(Gender.sum(1).astype(float), axis=0).plot(kind="bar", stacked=True, figsize=(4,4))

# Adding labels
plt.xlabel('Gender')
plt.ylabel('Percentage')

# Display the plot
plt.show()

print(df.groupby('Loan_Status')['ApplicantIncome'].mean())

df.groupby('Loan_Status')['ApplicantIncome'].mean().plot.bar()

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

classifier = DecisionTreeClassifier()
classifier.fit(X_train, Y_train)

X_test_pred = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_pred, Y_test)

print("Decision Tree Accuracy:", test_data_accuracy)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

classifier = RandomForestClassifier()
classifier.fit(X_train, Y_train)

X_test_pred = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_pred, Y_test)

print("Random Forest Accuracy:", test_data_accuracy)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

classifier = LogisticRegression()
classifier.fit(X_train, Y_train)

X_test_pred = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_pred, Y_test)

print("Logistic Regression Accuracy:", test_data_accuracy)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

classifier = KNeighborsClassifier()
classifier.fit(X_train, Y_train)

X_test_pred = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_pred, Y_test)

print("KNN Accuracy:", test_data_accuracy)

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

classifier = GaussianNB()
classifier.fit(X_train, Y_train)

X_test_pred = classifier.predict(X_test)
test_data_accuracy = accuracy_score(X_test_pred, Y_test)

print("Naive Bayes Accuracy:", test_data_accuracy)

from sklearn.metrics import accuracy_score

# Predict on the training data
X_train_pred = classifier.predict(X_train)

# Calculate and print the accuracy on the training data
training_data_accuracy = accuracy_score(Y_train, X_train_pred)
print(f"Training Data Accuracy: {training_data_accuracy:.2f}")

# Predict on the test data
X_test_pred = classifier.predict(X_test)

# Calculate and print the accuracy on the test data
test_data_accuracy = accuracy_score(Y_test, X_test_pred)
print(f"Test Data Accuracy: {test_data_accuracy:.2f}")

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Predict on the training data
X_train_pred = classifier.predict(X_train)

# Accuracy, Precision, Recall, F1-score on training data
training_data_accuracy = accuracy_score(Y_train, X_train_pred)
training_precision = precision_score(Y_train, X_train_pred)
training_recall = recall_score(Y_train, X_train_pred)
training_f1 = f1_score(Y_train, X_train_pred)

# Confusion Matrix for training data
training_conf_matrix = confusion_matrix(Y_train, X_train_pred)

# Print training set results
print(f"Training Data Accuracy: {training_data_accuracy:.2f}")
print(f"Training Data Precision: {training_precision:.2f}")
print(f"Training Data Recall: {training_recall:.2f}")
print(f"Training Data F1-Score: {training_f1:.2f}")
print("Training Confusion Matrix:")
print(training_conf_matrix)
print("\n" + "-"*50 + "\n")

# Predict on the test data
X_test_pred = classifier.predict(X_test)

# Accuracy, Precision, Recall, F1-score on test data
test_data_accuracy = accuracy_score(Y_test, X_test_pred)
test_precision = precision_score(Y_test, X_test_pred)
test_recall = recall_score(Y_test, X_test_pred)
test_f1 = f1_score(Y_test, X_test_pred)

# Confusion Matrix for test data
test_conf_matrix = confusion_matrix(Y_test, X_test_pred)

# Print test set results
print(f"Test Data Accuracy: {test_data_accuracy:.2f}")
print(f"Test Data Precision: {test_precision:.2f}")
print(f"Test Data Recall: {test_recall:.2f}")
print(f"Test Data F1-Score: {test_f1:.2f}")
print("Test Confusion Matrix:")
print(test_conf_matrix)

# Import necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Assuming y_cv is the true labels and pred_cv are the predicted labels
cm = confusion_matrix(X_test_pred, Y_test)

# Print the confusion matrix
print("Confusion Matrix:")
print(cm)

# Plotting the confusion matrix as a heatmap
plt.figure(figsize=(9, 6))  # Adjust the size of the plot
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues')  # Annotate the matrix with integer values
plt.title('Confusion Matrix of the Classifier')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# import classification_report
from sklearn.metrics import classification_report
print(classification_report(Y_test, X_test_pred))

"""The performance of our model seems encouraging, with accuracy of 82%, precision of 80% and recall of 95%.

```
# This is formatted as code
```



Letâ€™s make predictions for the test dataset and submit the result.
"""

import matplotlib.pyplot as plt

# Assuming training_data_accuracy and test_data_accuracy are already calculated
# and contain the accuracy values for training and test sets

# Data to plot
accuracy_values = [training_data_accuracy, test_data_accuracy]
labels = ['Training Accuracy', 'Test Accuracy']

# Plotting the bar chart
plt.figure(figsize=(8, 6))  # Set the size of the plot
plt.bar(labels, accuracy_values, color=['blue', 'green'])

# Add title and labels
plt.title('Accuracy Comparison: Training vs Test Data')
plt.xlabel('Data Set')
plt.ylabel('Accuracy Score')

# Display the accuracy values on top of the bars
for i, v in enumerate(accuracy_values):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', fontsize=12)

# Show the plot
plt.show()

import matplotlib.pyplot as plt

# Assuming you have already calculated training_data_accuracy and test_data_accuracy

# Data to plot
accuracy_values = [training_data_accuracy, test_data_accuracy]
labels = ['Training Accuracy', 'Test Accuracy']

# Plotting the bar chart
plt.figure(figsize=(8, 6))  # Set the size of the plot
plt.bar(labels, accuracy_values, color=['blue', 'green'])

# Add title and labels
plt.title('Accuracy Comparison: Training vs Test Data')
plt.xlabel('Data Set')
plt.ylabel('Accuracy Score')

# Display the accuracy values on top of the bars
for i, v in enumerate(accuracy_values):
    plt.text(i, v + 0.01, f'{v:.2f}', ha='center', fontsize=12)

# Show the plot
plt.show()

filename = 'loan_status_model.pkl'  # Name the .pkl file
with open(filename, 'wb') as f:
    pickle.dump(classifier, f)

print(f"Model saved as {filename}")

import pickle
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

# Step 1: Load dataset (Iris dataset as an example)
data = load_iris()
X = data.data
y = data.target

# Step 2: Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Define and train the model (RandomForestClassifier in this example)
model = RandomForestClassifier(n_estimators=100)
model.fit(X_train, y_train)

# Step 4: Save the trained model as a .pkl file
filename = 'loan_status_model.pkl'
with open(filename, 'wb') as f:
    pickle.dump(model, f)

print(f"Model saved as {filename}")

from google.colab import files

# Download the model.pkl file
files.download('loan_status_model.pkl')

